{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries required in the script\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil, os\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from keras.layers import Dense\n",
    "import sys\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import callbacks\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---  dataset 1 --- #\n",
    "\n",
    "# inpumetersSet excel file path (first training dataset)\n",
    "inputExcelFile =\"simulations.xlsx\"\n",
    "\n",
    "# Reading an excel file\n",
    "excelFile = pd.read_excel(inputExcelFile)\n",
    "\n",
    "# Converting excel file into CSV file\n",
    "excelFile.to_csv(\"dataset1.csv\", index = None, header=True)\n",
    "\n",
    "# Reading and Converting the output csv file into a dataframe object\n",
    "dataframeObject_1 = pd.DataFrame(pd.read_csv(\"dataset1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "Dense_1           = 32\n",
    "Dense_2           = 64\n",
    "Dense_3           = 32\n",
    "\n",
    "learning_rate     = 5e-4\n",
    "N                 = 280\n",
    "validation_split  = 0.2\n",
    "N_test            = 30  \n",
    "batch_size        = 60\n",
    "epochs            = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 1\n",
    "\n",
    "input_df_1 = dataframeObject_1.drop(['Simulation', 'A1', 'A2', 'B1', 'B2', 'kg', 'g', 'C1', 'Ap'], axis=1)\n",
    "\n",
    "output_df_1 = dataframeObject_1.drop(['Simulation', 'd10', 'd21', 'd32', 'd43', 'Concentration'], axis=1)\n",
    "\n",
    "input_array_1 = input_df_1.to_numpy()\n",
    "\n",
    "output_array_1 = output_df_1.to_numpy()\n",
    "\n",
    "input_array_1[:,[1,2,3,4]] = np.log10(input_array_1[:,[1,2,3,4]])\n",
    "\n",
    "output_array_1[:,[0,1,4,7]] = np.log10(output_array_1[:,[0,1,4,7]])\n",
    "\n",
    "# scaling dataset\n",
    "\n",
    "scaler_X_K = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "x_training_scaled_1 = scaler_X_K.fit(input_array_1)\n",
    "x_training_scaled_1 = scaler_X_K.transform(input_array_1)\n",
    "\n",
    "scaler_y_K = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "y_training_scaled_1 = scaler_y_K.fit(output_array_1)\n",
    "y_training_scaled_1 = scaler_y_K.transform(output_array_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random splitting\n",
    "x_training, x_testing, y_training, y_testing = train_test_split(x_training_scaled_1, y_training_scaled_1, test_size=validation_split)\n",
    "\n",
    "print(x_training.shape)\n",
    "print(y_training.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "initializer = 'normal'\n",
    "\n",
    "model.add(Dense(Dense_1, input_dim=x_training.shape[1], kernel_initializer=initializer, activation='relu'))\n",
    "model.add(Dense(Dense_2, kernel_initializer=initializer, activation='relu'))\n",
    "model.add(Dense(Dense_3, kernel_initializer=initializer, activation='relu'))\n",
    "model.add(Dense(y_training.shape[1], kernel_initializer=initializer, activation=\"linear\"))\n",
    "\n",
    "# compile the model with loss and optimizer\n",
    "model.compile(loss=keras.losses.mean_absolute_error,\n",
    "              optimizer=Adam(learning_rate=learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# save model at the best performance, early stopping if the accuracy on validation does not change\n",
    "checkpoint = ModelCheckpoint(\"best_model_K.hdf5\", monitor='val_loss', verbose=1,\n",
    "             save_best_only=True, mode='auto')\n",
    "es = EarlyStopping(monitor='val_loss', verbose=1, patience=5000)\n",
    "\n",
    "class PlotCurrentEstimate(callbacks.Callback):\n",
    "    \"\"\"Callback to plot loss value and layers while training\"\"\"    \n",
    "    def __init__(self, update_freq=2):\n",
    "        self.epoch = 0\n",
    "        self.update_freq = update_freq\n",
    "        self.h = {'loss': [], \"val_loss\": []}        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epoch += 1\n",
    "        \n",
    "        if self.epoch % self.update_freq == 0:\n",
    "            y_curr = self.model.predict(x_training)\n",
    "            clear_output(wait=True)\n",
    "           \n",
    "            self.h[\"loss\"].append(logs[\"loss\"])\n",
    "            self.h[\"val_loss\"].append(logs[\"val_loss\"])\n",
    "            \n",
    "            h = self.h\n",
    "\n",
    "trainplot = PlotCurrentEstimate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_training, y_training,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_data=(x_testing, y_testing),\n",
    "          callbacks=[checkpoint, es, trainplot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(\"trainingCurve.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_training = model.predict(x_training)\n",
    "y_predicted_training = scaler_y_K.inverse_transform(y_predicted_training)\n",
    "y_training_rescale = scaler_y_K.inverse_transform(y_training)\n",
    "\n",
    "y_predicted_test = model.predict(x_testing)\n",
    "y_predicted_test = scaler_y_K.inverse_transform(y_predicted_test)\n",
    "y_testing_rescale = scaler_y_K.inverse_transform(y_testing)\n",
    "\n",
    "# the error is calculated between the real y_testing values and NN predicted ones\n",
    "errore = np.zeros(y_predicted_test.shape)\n",
    "errore_medio = []\n",
    "for j in range(0,y_predicted_test.shape[1]):\n",
    "  for i in range(0,y_predicted_test.shape[0]):\n",
    "    errore[i][j] = float(abs((y_testing_rescale[i][j]-y_predicted_test[i][j])/y_testing_rescale[i][j]))*100\n",
    "  errore_medio.append(sum(np.abs(errore[:,j]))/len(errore[:,j]))\n",
    "errore_massimo = np.max(errore)\n",
    "errore_minimo = np.min(errore)\n",
    "print('Average percentage error on test set = %r\\nMaximum percentage error= %r\\nMinimum percentage error = %r' %(errore_medio, errore_massimo, errore_minimo))\n",
    "dev = np.std(errore)\n",
    "\n",
    "# plotting results\n",
    "fig, axs = plt.subplots(2,4, figsize=(7, 7))\n",
    "fig.subplots_adjust(hspace = .3, wspace=.5)\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in range(y_testing_rescale.shape[1]):\n",
    "    axs[i].plot(y_testing_rescale[:,i], y_predicted_test[:,i], 'ro')\n",
    "    axs[i].plot(y_testing_rescale[:,i], y_testing_rescale[:,i])\n",
    "    if i==0:\n",
    "        axs[i].set_ylabel('NN')\n",
    "    elif i==4:\n",
    "        axs[i].set_xlabel('CFD')\n",
    "        axs[i].set_ylabel('NN')\n",
    "    elif i==7:\n",
    "        axs[i].set_xlabel('CFD')\n",
    "\n",
    "fig.savefig(\"testing.png\")\n",
    "\n",
    "fig, axs = plt.subplots(2,4, figsize=(7, 7))\n",
    "fig.subplots_adjust(hspace = .3, wspace=.5)\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in range(y_testing_rescale.shape[1]):\n",
    "    axs[i].plot(y_training_rescale[:,i], y_predicted_training[:,i], 'bo')\n",
    "    axs[i].plot(y_training_rescale[:,i], y_training_rescale[:,i])\n",
    "    if i==0:\n",
    "        axs[i].set_ylabel('NN')\n",
    "    elif i==2:\n",
    "        axs[i].set_xlabel('CFD')\n",
    "        axs[i].set_ylabel('NN')\n",
    "    elif i==3:\n",
    "        axs[i].set_xlabel('CFD')\n",
    "\n",
    "fig.savefig(\"training.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
